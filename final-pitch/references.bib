@InProceedings{approximate-state-abstraction,
  title = 	 {Near Optimal Behavior via Approximate State Abstraction},
  author = 	 {Abel, David and Hershkowitz, David and Littman, Michael},
  booktitle = 	 {Proceedings of The 33rd International Conference on Machine Learning},
  pages = 	 {2915--2923},
  year = 	 {2016},
  editor = 	 {Balcan, Maria Florina and Weinberger, Kilian Q.},
  volume = 	 {48},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {New York, New York, USA},
  month = 	 {20--22 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v48/abel16.pdf},
  url = 	 {https://proceedings.mlr.press/v48/abel16.html},
  abstract = 	 {The combinatorial explosion that plagues planning and reinforcement learning (RL) algorithms can be moderated using state abstraction. Prohibitively large task representations can be condensed such that essential information is preserved, and consequently, solutions are tractably computable. However, exact abstractions, which treat only fully-identical situations as equivalent, fail to present opportunities for abstraction in environments where no two situations are exactly alike. In this work, we investigate approximate state abstractions, which treat nearly-identical situations as equivalent. We present theoretical guarantees of the quality of behaviors derived from four types of approximate abstractions. Additionally, we empirically demonstrate that approximate abstractions lead to reduction in task complexity and bounded loss of optimality of behavior in a variety of environments.}
}

@inproceedings{curriculum-learning,
author = {Bengio, Yoshua and Louradour, J\'{e}r\^{o}me and Collobert, Ronan and Weston, Jason},
title = {Curriculum learning},
year = {2009},
isbn = {9781605585161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1553374.1553380},
doi = {10.1145/1553374.1553380},
abstract = {Humans and animals learn much better when the examples are not randomly presented but organized in a meaningful order which illustrates gradually more concepts, and gradually more complex ones. Here, we formalize such training strategies in the context of machine learning, and call them "curriculum learning". In the context of recent research studying the difficulty of training in the presence of non-convex training criteria (for deep deterministic and stochastic neural networks), we explore curriculum learning in various set-ups. The experiments show that significant improvements in generalization can be achieved. We hypothesize that curriculum learning has both an effect on the speed of convergence of the training process to a minimum and, in the case of non-convex criteria, on the quality of the local minima obtained: curriculum learning can be seen as a particular form of continuation method (a general strategy for global optimization of non-convex functions).},
booktitle = {Proceedings of the 26th Annual International Conference on Machine Learning},
pages = {41–48},
numpages = {8},
location = {Montreal, Quebec, Canada},
series = {ICML '09}
}

@inproceedings{potential-rl,
author = {Ng, Andrew Y. and Harada, Daishi and Russell, Stuart J.},
title = {Policy Invariance Under Reward Transformations: Theory and Application to Reward Shaping},
year = {1999},
isbn = {1558606122},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
booktitle = {Proceedings of the Sixteenth International Conference on Machine Learning},
pages = {278–287},
numpages = {10},
series = {ICML '99}
}

@article{schulman2017proximal,
  title   = {Proximal Policy Optimization Algorithms},
  author  = {Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  journal = {arXiv preprint arXiv:1707.06347},
  year    = {2017}
}

@book{machinations,
  title={Engineering Emergence: Applied Theory for Game Design},
  author={Dormans, Joris},
  year={2012},
  publisher={Universiteit van Amsterdam},
  address={Amsterdam},
  isbn={9461907524, 9789461907523},
  url={https://dare.uva.nl/document/2/102091}
}

@book{rl,
  title={Reinforcement Learning: An Introduction},
  author={Sutton, Richard S. and Barto, Andrew G.},
  year={2018},
  publisher={A Bradford Book},
  address={Cambridge, MA, USA},
  isbn={9780262039246},
  url={http://incompleteideas.net/book/the-book-2nd.html}
}

@book{ai,
  title     = {Artificial Intelligence: A Modern Approach},
  author    = {Russell, Stuart J. and Norvig, Peter},
  year      = {2020},
  edition   = {4th},
  publisher = {Pearson},
  isbn      = {9780134610993}
}

@misc{monopoly,
  title        = {Monopoly},
  publisher    = {Parker Brothers},
  year         = {1935},
  note         = {Jogo de tabuleiro referenciado no estudo Engineering Emergence.}
}

@misc{catan,
  title        = {The Settlers of Catan},
  publisher    = {Kosmos (initial edition) / Mayfair Games (English edition)},
  year         = {1995},
  note         = {Jogo de tabuleiro referenciado no estudo Engineering Emergence.}
}


@misc{dqn,
      title={Playing Atari with Deep Reinforcement Learning}, 
      author={Volodymyr Mnih and Koray Kavukcuoglu and David Silver and Alex Graves and Ioannis Antonoglou and Daan Wierstra and Martin Riedmiller},
      year={2013},
      eprint={1312.5602},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1312.5602}, 
}

@inproceedings{rrl,
  title     = {Relational Reinforcement Learning: An Overview},
  author    = {Tadepalli, Prasad and Givan, Robert and Driessens, Kurt},
  booktitle = {Proceedings of the ICML-2004 Workshop on Relational Reinforcement Learning},
  year      = {2004},
  month     = {Jan},
  pages     = {1--9}
}

@article{modeling-rl-games,
title = {A modeling environment for reinforcement learning in games},
journal = {Entertainment Computing},
volume = {43},
pages = {100516},
year = {2022},
issn = {1875-9521},
doi = {https://doi.org/10.1016/j.entcom.2022.100516},
url = {https://www.sciencedirect.com/science/article/pii/S1875952122000404},
author = {Gilzamir Gomes and Creto A. Vidal and Joaquim B. Cavalcante-Neto and Yuri L.B. Nogueira},
keywords = {Reinforcement Learning, Games, Non-Player Characters, Navigation Problem},
abstract = {Developing Non-Player Characters, i.e., game characters that interact with the game’s environment autonomously with flexibility to experiment different behavior configurations is not a trivial task. Traditionally, this has been done with techniques that limit the complexity of the behavior of Non-Player Characters (NPCs), as in the case of using Navigation Mesh (NavMesh) for navigation behaviors. For this problem, it has been shown that reinforcement learning can be more efficient and flexible than traditional techniques. However, integrating reinforcement learning into current game development tools is laborious, given that a great deal of experimentation and coding is required. For that, we have developed a modeling environment that integrates with a game development tool and allows the direct specification of reward functions and NPC agent components with maximum code reuse and automatic code generation.}
}

