@inproceedings{potential-rl,
author = {Ng, Andrew Y. and Harada, Daishi and Russell, Stuart J.},
title = {Policy Invariance Under Reward Transformations: Theory and Application to Reward Shaping},
year = {1999},
isbn = {1558606122},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
booktitle = {Proceedings of the Sixteenth International Conference on Machine Learning},
pages = {278–287},
numpages = {10},
series = {ICML '99}
}

@article{schulman2017proximal,
  title   = {Proximal Policy Optimization Algorithms},
  author  = {Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  journal = {arXiv preprint arXiv:1707.06347},
  year    = {2017}
}


@article{openai-hand,
    author = {OpenAI: Marcin Andrychowicz and Bowen Baker and Maciek Chociej and Rafal Józefowicz and Bob McGrew and Jakub Pachocki and Arthur Petron and Matthias Plappert and Glenn Powell and Alex Ray and Jonas Schneider and Szymon Sidor and Josh Tobin and Peter Welinder and Lilian Weng and Wojciech Zaremba},
    title ={Learning dexterous in-hand manipulation},
    journal = {The International Journal of Robotics Research},
    volume = {39},
    number = {1},
    pages = {3-20},
    year = {2020},
    doi = {10.1177/0278364919887447},
    URL = { 
        
            https://doi.org/10.1177/0278364919887447
        
        

    },
    eprint = { 
            https://doi.org/10.1177/0278364919887447
    }
}

@inproceedings{arbitrary-reward-functions,
  title={Expressing arbitrary reward functions as potential-based advice},
  author={Harutyunyan, Aram and Devlin, Simon and Vrancx, Tom and Now{\'e}, Ann},
  booktitle={Proceedings of the 29th AAAI Conference on Artificial Intelligence},
  pages={2652--2658},
  year={2015}
}

@article{meta-learning,
  title={Reward Shaping via Meta-Learning},
  author={Zou, Haosheng and Ren, Tongzheng and Yan, Dong and Su, Hang and Zhu, Jun},
  journal={arXiv preprint arXiv:1901.09330},
  year={2019}
}

@inproceedings{gail,
 author = {Ho, Jonathan and Ermon, Stefano},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Lee and M. Sugiyama and U. Luxburg and I. Guyon and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Generative Adversarial Imitation Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/2016/file/cc7e2b878868cbae992d1fb743995d8f-Paper.pdf},
 volume = {29},
 year = {2016}
}

@article{deepmimic,
	author = {Peng, Xue Bin and Abbeel, Pieter and Levine, Sergey and van de Panne, Michiel},
	title = {DeepMimic: Example-guided Deep Reinforcement Learning of Physics-based Character Skills},
	journal = {ACM Trans. Graph.},
	issue_date = {August 2018},
	volume = {37},
	number = {4},
	month = jul,
	year = {2018},
	issn = {0730-0301},
	pages = {143:1--143:14},
	articleno = {143},
	numpages = {14},
	url = {http://doi.acm.org/10.1145/3197517.3201311},
	doi = {10.1145/3197517.3201311},
	acmid = {3201311},
	publisher = {ACM},
	address = {New York, NY, USA},
	keywords = {motion control, physics-based character animation, reinforcement learning},
} 

@inproceedings{advising,
    author = {Wiewiora, Eric and Cottrell, Garrison and Elkan, Charles},
    title = {Principled methods for advising reinforcement learning agents},
    year = {2003},
    isbn = {1577351894},
    publisher = {AAAI Press},
    abstract = {An important issue in reinforcement learning is how to incorporate expert knowledge in a principled manner, especially as we scale up to real-world tasks. In this paper, we present a method for incorporating arbitrary advice into the reward structure of a reinforcement learning agent without altering the optimal policy. This method extends the potential-based shaping method proposed by Ng et al. (1999) to the case of shaping functions based on both states and actions. This allows for much more specific information to guide the agent - which action to choose - without requiring the agent to discover this from the rewards on states alone. We develop two qualitatively different methods for converting a potential function into advice for the agent. We also provide theoretical and experimental justifications for choosing between these advice-giving algorithms based on the properties of the potential function.},
    booktitle = {Proceedings of the Twentieth International Conference on International Conference on Machine Learning},
    pages = {792–799},
    numpages = {8},
    location = {Washington, DC, USA},
    series = {ICML'03}
}

@book{machinations,
  title={Engineering Emergence: Applied Theory for Game Design},
  author={Dormans, Joris},
  year={2012},
  publisher={Universiteit van Amsterdam},
  address={Amsterdam},
  isbn={9461907524, 9789461907523},
  url={https://dare.uva.nl/document/2/102091}
}

@book{rl,
  title={Reinforcement Learning: An Introduction},
  author={Sutton, Richard S. and Barto, Andrew G.},
  year={2018},
  publisher={A Bradford Book},
  address={Cambridge, MA, USA},
  isbn={9780262039246},
  url={http://incompleteideas.net/book/the-book-2nd.html}
}

@book{ai,
  title     = {Artificial Intelligence: A Modern Approach},
  author    = {Russell, Stuart J. and Norvig, Peter},
  year      = {2020},
  edition   = {4th},
  publisher = {Pearson},
  isbn      = {9780134610993}
}

@misc{monopoly,
  title        = {Monopoly},
  publisher    = {Parker Brothers},
  year         = {1935},
  note         = {Jogo de tabuleiro referenciado no estudo Engineering Emergence.}
}

@misc{catan,
  title        = {The Settlers of Catan},
  publisher    = {Kosmos (initial edition) / Mayfair Games (English edition)},
  year         = {1995},
  note         = {Jogo de tabuleiro referenciado no estudo Engineering Emergence.}
}

@book{who,
  title        = {COMPLETE THIS},
  year         = {1984},
  author       = {MissingCitation}
}
